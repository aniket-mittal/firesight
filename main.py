# -*- coding: utf-8 -*-
"""Complete Code - Wildfire Prediction Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DkgCx6WjrmhgcBvnwuEhaSakB6I0f0q4

#Wildfire Prediction Project
Aniket Mittal

Note: All comments in this file were generated by ChatGPT (original file from collab is what I worked on).

##Connect to Colab.
"""

# Mount Google Drive to access datasets
from google.colab import drive
drive.mount("/content/drive")

"""## Create, parse and concatenate datasets."""

# Import required libraries for data processing and visualization
import tensorflow as tf
from typing import Dict, List, Optional, Text, NamedTuple, Tuple
import matplotlib.pyplot as plt
from matplotlib import colors
import seaborn as sns
import numpy as np
import pandas as pd
from functools import reduce

# Define file patterns for training datasets (15 training datasets, 1 validation, 1 test)
file_pattern_dataset_1 = "/content/drive/MyDrive/Training_Data/Dataset_1/train*"
file_pattern_dataset_2 = "/content/drive/MyDrive/Training_Data/Dataset_2/train*"
file_pattern_dataset_3 = "/content/drive/MyDrive/Training_Data/Dataset_3/train*"
file_pattern_dataset_4 = "/content/drive/MyDrive/Training_Data/Dataset_4/train*"
file_pattern_dataset_5 = "/content/drive/MyDrive/Training_Data/Dataset_5/train*"
file_pattern_dataset_6 = "/content/drive/MyDrive/Training_Data/Dataset_6/train*"
file_pattern_dataset_7 = "/content/drive/MyDrive/Training_Data/Dataset_7/train*"
file_pattern_dataset_8 = "/content/drive/MyDrive/Training_Data/Dataset_8/train*"
file_pattern_dataset_9 = "/content/drive/MyDrive/Training_Data/Dataset_9/train*"
file_pattern_dataset_10 = "/content/drive/MyDrive/Training_Data/Dataset_10/train*"
file_pattern_dataset_11 = "/content/drive/MyDrive/Training_Data/Dataset_11/train*"
file_pattern_dataset_12 = "/content/drive/MyDrive/Training_Data/Dataset_12/train*"
file_pattern_dataset_13 = "/content/drive/MyDrive/Training_Data/Dataset_13/train*"
file_pattern_dataset_14 = "/content/drive/MyDrive/Training_Data/Dataset_14/train*"
file_pattern_dataset_15 = "/content/drive/MyDrive/Training_Data/Dataset_15/train*"
file_pattern_validation_dataset = "/content/drive/MyDrive/Test_Data/eval*"
file_pattern_test_dataset = "/content/drive/MyDrive/Test_Data/test*"

# Define input features for the model:
# - elevation: terrain height
# - th: temperature humidity index
# - vs: wind speed
# - tmmn: minimum temperature
# - tmmx: maximum temperature
# - sph: specific humidity
# - pr: precipitation
# - pdsi: Palmer drought severity index
# - NDVI: normalized difference vegetation index
# - population: population density
# - erc: energy release component
# - PrevFireMask: previous fire mask
INPUT_FEATURES = ['elevation', 'th', 'vs', 'tmmn', 'tmmx', 'sph',
                  'pr', 'pdsi', 'NDVI', 'population', 'erc', 'PrevFireMask']
# Output feature: binary mask indicating fire presence
OUTPUT_FEATURES = ['FireMask']

# Function to get feature descriptions for TFRecord parsing
# Creates a dictionary mapping feature names to their TFRecord format specifications
def _get_features_desc(
        sample_size: int,
        features: List[Text]) -> Dict[Text, tf.io.FixedLenFeature]:
    sample_shape = [sample_size, sample_size]  # 2D grid shape
    features = set(features)  # Remove duplicates
    columns = [
        tf.io.FixedLenFeature(shape=sample_shape, dtype=tf.float32)
        for _ in features
    ]
    return dict(zip(features, columns))

# Function to parse individual TFRecord examples
# Converts raw TFRecord data into tensors with proper shapes
def _parse_function(
        example_proto: tf.train.Example,
        data_size: int
) -> Dict[Text, tf.Tensor]:
    features_desc = \
        _get_features_desc(data_size, INPUT_FEATURES + OUTPUT_FEATURES)
    features_dict = tf.io.parse_single_example(example_proto, features_desc)
    return features_dict

# Function to create and preprocess dataset from file pattern
# Handles file loading, parsing, and optimization
def get_dataset(
        file_pattern: Text,
        data_size: int
) -> tf.data.Dataset:
    # List all files matching the pattern
    dataset = tf.data.Dataset.list_files(file_pattern)
    # Interleave files for better performance
    dataset = dataset.interleave(
        lambda x: tf.data.TFRecordDataset(x, compression_type=None),
        num_parallel_calls=tf.data.experimental.AUTOTUNE)
    # Prefetch data for better performance
    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)
    # Parse each example
    dataset = dataset.map(
        lambda x: _parse_function(x, data_size),
        num_parallel_calls=tf.data.experimental.AUTOTUNE)
    return dataset

# Create datasets for training, validation and testing
# Each dataset is loaded and parsed using the get_dataset function
dataset_list = [
    get_dataset(file_pattern=globals()[f"file_pattern_dataset_{i}"], data_size=64)
    for i in range(1, 16)
]
validation_dataset = get_dataset(file_pattern=file_pattern_validation_dataset, data_size=64)
test_dataset = get_dataset(file_pattern=file_pattern_test_dataset, data_size=64)

# Function to parse features into input and output tensors
# Converts dictionary of features into properly shaped tensors for model input/output
def parse_function_2(features: Dict) -> Tuple[tf.Tensor, tf.Tensor]:
    INPUT_FEATURES = ['elevation', 'th', 'vs',  'tmmn', 'tmmx', 'sph',
                  'pr', 'pdsi', 'NDVI', 'population', 'erc', 'PrevFireMask']
    OUTPUT_FEATURES = ['FireMask', ]
    input_features, output_features = INPUT_FEATURES, OUTPUT_FEATURES
    # Stack input features along channel dimension
    inputs_list = [features.get(key) for key in input_features]
    inputs_stacked = tf.stack(inputs_list, axis=0)
    input_img = tf.transpose(inputs_stacked, [1, 2, 0])
    # Stack output features
    outputs_list = [features.get(key) for key in output_features]
    outputs_stacked = tf.stack(outputs_list, axis=0)
    output_img = tf.transpose(outputs_stacked, [1, 2, 0])
    return input_img, output_img

# Combine all datasets and apply parsing
# Concatenates training, validation, and test datasets
dataset = reduce(lambda ds1, ds2: ds1.concatenate(ds2), dataset_list + [test_dataset, validation_dataset])
dataset = dataset.map(lambda x: parse_function_2(x), num_parallel_calls=tf.data.experimental.AUTOTUNE)

# Create k-fold cross validation splits
# Divides data into k=5 folds for cross-validation
k=5
num_samples = 18545
fold_size = num_samples // k
folds = []
for i in range(k):
    fold = dataset.skip(i * fold_size).take(fold_size)
    folds.append(fold)

"""## Create and implement ResNet50 Model"""

import pickle
# Function to create ResNet50 model with 12 input channels
# Architecture:
# 1. Input layer (64x64x12) - Takes 12 environmental features as input
# 2. ResNet50 base model - Uses pre-trained ResNet50 architecture without top layers
# 3. Global average pooling - Reduces spatial dimensions
# 4. Dense layer with sigmoid activation - Produces probability map
# 5. Reshape to output shape (64x64x1) - Final fire prediction mask
def resnet50_12channel(input_shape):
    # Create input tensor with specified shape
    input_tensor = tf.keras.layers.Input(shape=input_shape)

    # Split input into 12 channels for each environmental feature
    channels = tf.split(input_tensor, num_or_size_splits=12, axis=-1)

    # Load ResNet50 base model without top layers
    # weights=None means we train from scratch
    base_model = tf.keras.applications.resnet.ResNet50(include_top=False, input_tensor=input_tensor, weights=None)

    # Add custom top layers for fire prediction
    x = base_model.output
    x = tf.keras.layers.GlobalAveragePooling2D()(x)  # Reduce spatial dimensions
    x = tf.keras.layers.Dense(64*64, activation='sigmoid')(x)  # Generate probability map
    x = tf.keras.layers.Reshape((64, 64, 1))(x)  # Reshape to match output dimensions

    # Create and return the complete model
    model = tf.keras.models.Model(inputs=input_tensor, outputs=x)
    return model

# Initialize evaluation metrics:
# - accuracy: overall prediction accuracy
# - precision: true positives / (true positives + false positives)
# - auc: area under ROC curve (measures model's ability to distinguish classes)
# - recall: true positives / (true positives + false negatives)
# - binary_accuracy: accuracy for binary classification
# - mae: mean absolute error (measures average prediction error)
# - auc_pr: area under precision-recall curve (better for imbalanced data)
accuracy = tf.keras.metrics.Accuracy()
precision = tf.keras.metrics.Precision()
auc = tf.keras.metrics.AUC()
recall = tf.keras.metrics.Recall()
binary_accuracy = tf.keras.metrics.BinaryAccuracy()
mae = tf.keras.metrics.MeanAbsoluteError()
auc_pr = tf.keras.metrics.AUC(curve='PR')

# Train and evaluate ResNet50 model using k-fold cross validation
# This helps ensure robust model evaluation by testing on different data splits
for i in range(k):
    print(f'Fold {i + 1}:')
    history_resnet50 = tf.keras.callbacks.History()
    
    # Prepare training and validation data for current fold
    # Use 4 folds for training and 1 fold for validation
    not_i = [j for j in range(5) if j != i]
    validation_data = folds[i]
    training_data = folds[not_i[0]]
    training_data = training_data.concatenate(folds[not_i[1]])
    training_data = training_data.concatenate(folds[not_i[2]])
    training_data = training_data.concatenate(folds[not_i[3]])
    
    # Batch and prefetch data for better performance
    # Batch size of 100 balances memory usage and training speed
    training_data = training_data.batch(100)
    training_data = training_data.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)
    validation_data = validation_data.batch(100)
    validation_data = validation_data.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)
    
    # Create and compile model
    model = resnet50_12channel((64,64,12))
    # Use binary crossentropy loss for binary classification
    # Include multiple metrics for comprehensive evaluation
    model.compile(optimizer='adam', loss='binary_crossentropy', 
                 metrics=[accuracy, precision, auc, recall, binary_accuracy, mae])
    
    # Train model for 75 epochs
    # Use validation data to monitor overfitting
    model.fit(training_data, epochs=75, validation_data=validation_data, 
             callbacks=[history_resnet50])
    
    # Evaluate model on validation data
    for batch in validation_data:
        inputs, targets = batch
        predictions = model.predict(inputs)
        targets = targets.numpy()
        predictions = predictions
        
        # Update all evaluation metrics
        binary_accuracy.update_state(targets, predictions)
        auc.update_state(targets, predictions)
        auc_pr.update_state(targets, predictions)
        mae.update_state(targets, predictions)
        precision.update_state(targets, predictions)
        recall.update_state(targets, predictions)
    
    # Print evaluation metrics for current fold
    print("Binary Accuracy: ", binary_accuracy.result().numpy())
    print("AUC PR: ", auc_pr.result().numpy())
    print("AUC: ", auc.result().numpy())
    print("MAE: ", mae.result().numpy())
    print("Precision: ", precision.result().numpy())
    print("Recall: ", recall.result().numpy())
    
    # Plot training history for current fold
    # Show both training and validation MAE
    plt.plot(history_resnet50.history['mean_absolute_error'])
    plt.plot(history_resnet50.history['val_mean_absolute_error'])
    plt.xlabel('Epochs')
    plt.ylabel('MAE Loss')
    plt.ylim(0,0.2)  # Set y-axis limit for better visualization
    plt.legend(['Train', 'Test'], loc="upper right")
    plt.title("Epochs vs. MAE Loss: ResNet50")
    plt.show()
    plt.close()
    
    # Save training history for first fold
    # This will be used later for model comparison
    if i == 0:
      with open('/content/drive/MyDrive/Data_History/history_resnet.pickle', 'wb') as f:
        pickle.dump(history_resnet50.history, f)

"""## Create and implement ResNet101 Model"""

# Function to create ResNet101 model with 12 input channels
# Similar to ResNet50 but with more layers for potentially better feature extraction
def resnet101_12channel(input_shape):
    input_tensor = tf.keras.layers.Input(shape=input_shape)

    # Split input into 12 channels for each environmental feature
    channels = tf.split(input_tensor, num_or_size_splits=12, axis=-1)

    # Load ResNet101 base model without top layers
    base_model = tf.keras.applications.resnet.ResNet101(include_top=False, input_tensor=input_tensor, weights=None)

    # Add custom top layers for fire prediction
    x = base_model.output
    x = tf.keras.layers.GlobalAveragePooling2D()(x)
    x = tf.keras.layers.Dense(64*64, activation='sigmoid')(x)
    x = tf.keras.layers.Reshape((64, 64, 1))(x)

    model = tf.keras.models.Model(inputs=input_tensor, outputs=x)
    return model

# Initialize metrics for ResNet101 evaluation
accuracy = tf.keras.metrics.Accuracy()
precision = tf.keras.metrics.Precision()
auc = tf.keras.metrics.AUC()
recall = tf.keras.metrics.Recall()
binary_accuracy = tf.keras.metrics.BinaryAccuracy()
mae = tf.keras.metrics.MeanAbsoluteError()
auc_pr = tf.keras.metrics.AUC(curve='PR')

# Train and evaluate ResNet101 model using k-fold cross validation
for i in range(k):
    print(f'Fold {i + 1}:')
    history_resnet101 = tf.keras.callbacks.History()
    
    # Prepare data for current fold
    not_i = [j for j in range(5) if j != i]
    validation_data = folds[i]
    training_data = folds[not_i[0]]
    training_data = training_data.concatenate(folds[not_i[1]])
    training_data = training_data.concatenate(folds[not_i[2]])
    training_data = training_data.concatenate(folds[not_i[3]])
    
    # Optimize data pipeline
    training_data = training_data.batch(100)
    training_data = training_data.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)
    validation_data = validation_data.batch(100)
    validation_data = validation_data.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)
    
    # Create and compile model
    model = resnet101_12channel((64,64,12))
    model.compile(optimizer='adam', loss='binary_crossentropy', 
                 metrics=[accuracy, precision, auc, recall, binary_accuracy, mae])
    
    # Train model
    model.fit(training_data, epochs=75, validation_data=validation_data, 
             callbacks=[history_resnet101])
    
    # Evaluate model
    for batch in validation_data:
        inputs, targets = batch
        predictions = model.predict(inputs)
        targets = targets.numpy()
        predictions = predictions
        
        # Update metrics
        binary_accuracy.update_state(targets, predictions)
        auc.update_state(targets, predictions)
        auc_pr.update_state(targets, predictions)
        mae.update_state(targets, predictions)
        precision.update_state(targets, predictions)
        recall.update_state(targets, predictions)
    
    # Print metrics
    print("Binary Accuracy: ", binary_accuracy.result().numpy())
    print("AUC PR: ", auc_pr.result().numpy())
    print("AUC: ", auc.result().numpy())
    print("MAE: ", mae.result().numpy())
    print("Precision: ", precision.result().numpy())
    print("Recall: ", recall.result().numpy())
    
    # Plot training history
    plt.plot(history_resnet101.history['mean_absolute_error'])
    plt.plot(history_resnet101.history['val_mean_absolute_error'])
    plt.xlabel('Epochs')
    plt.ylabel('MAE Loss')
    plt.ylim(0,0.2)
    plt.legend(['Train', 'Test'], loc="upper right")
    plt.title("Epochs vs. MAE Loss: ResNet101")
    plt.show()
    plt.close()
    
    # Save history for first fold
    if i == 0:
      with open('/content/drive/MyDrive/Data_History/history_resnet101.pickle', 'wb') as f:
        pickle.dump(history_resnet101.history, f)

"""## Create and implement EfficientNet Model"""

# Function to create EfficientNet model with 12 input channels
# EfficientNet is designed to be more efficient in terms of parameters and computation
def efficientnet_12channel(input_shape):
    input_tensor = tf.keras.layers.Input(shape=input_shape)

    # Split input into 12 channels
    channels = tf.split(input_tensor, num_or_size_splits=12, axis=-1)

    # Load EfficientNetB0 base model
    # B0 is the smallest variant, good for faster training
    base_model = tf.keras.applications.efficientnet.EfficientNetB0(
        include_top=False, input_tensor=input_tensor, weights=None)

    # Add custom top layers
    x = base_model.output
    x = tf.keras.layers.GlobalAveragePooling2D()(x)
    x = tf.keras.layers.Dense(64*64, activation='sigmoid')(x)
    x = tf.keras.layers.Reshape((64, 64, 1))(x)

    model = tf.keras.models.Model(inputs=input_tensor, outputs=x)
    return model

# Initialize metrics for EfficientNet evaluation
accuracy = tf.keras.metrics.Accuracy()
precision = tf.keras.metrics.Precision()
auc = tf.keras.metrics.AUC()
recall = tf.keras.metrics.Recall()
binary_accuracy = tf.keras.metrics.BinaryAccuracy()
mae = tf.keras.metrics.MeanAbsoluteError()
auc_pr = tf.keras.metrics.AUC(curve='PR')

# Train and evaluate EfficientNet model
for i in range(k):
    print(f'Fold {i + 1}:')
    history_efficientnet = tf.keras.callbacks.History()
    
    # Prepare data for current fold
    not_i = [j for j in range(5) if j != i]
    validation_data = folds[i]
    training_data = folds[not_i[0]]
    training_data = training_data.concatenate(folds[not_i[1]])
    training_data = training_data.concatenate(folds[not_i[2]])
    training_data = training_data.concatenate(folds[not_i[3]])
    
    # Optimize data pipeline
    training_data = training_data.batch(100)
    training_data = training_data.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)
    validation_data = validation_data.batch(100)
    validation_data = validation_data.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)
    
    # Create and compile model
    model = efficientnet_12channel((64,64,12))
    model.compile(optimizer='adam', loss='binary_crossentropy', 
                 metrics=[accuracy, precision, auc, recall, binary_accuracy, mae])
    
    # Train model
    model.fit(training_data, epochs=75, validation_data=validation_data, 
             callbacks=[history_efficientnet])
    
    # Evaluate model
    for batch in validation_data:
        inputs, targets = batch
        predictions = model.predict(inputs)
        targets = targets.numpy()
        predictions = predictions
        
        # Update metrics
        binary_accuracy.update_state(targets, predictions)
        auc.update_state(targets, predictions)
        auc_pr.update_state(targets, predictions)
        mae.update_state(targets, predictions)
        precision.update_state(targets, predictions)
        recall.update_state(targets, predictions)
    
    # Print metrics
    print("Binary Accuracy: ", binary_accuracy.result().numpy())
    print("AUC PR: ", auc_pr.result().numpy())
    print("AUC: ", auc.result().numpy())
    print("MAE: ", mae.result().numpy())
    print("Precision: ", precision.result().numpy())
    print("Recall: ", recall.result().numpy())
    
    # Plot training history
    plt.plot(history_efficientnet.history['mean_absolute_error'])
    plt.plot(history_efficientnet.history['val_mean_absolute_error'])
    plt.xlabel('Epochs')
    plt.ylabel('MAE Loss')
    plt.ylim(0,0.2)
    plt.legend(['Train', 'Test'], loc="upper right")
    plt.title("Epochs vs. MAE Loss: EfficientNet")
    plt.show()
    plt.close()
    
    # Save history for first fold
    if i == 0:
      with open('/content/drive/MyDrive/Data_History/history_efficientnet.pickle', 'wb') as f:
        pickle.dump(history_efficientnet.history, f)

"""## EfficientNet V2S"""

# Function to create EfficientNetV2S model with 12 input channels
# EfficientNetV2S is an improved version of EfficientNet with better training efficiency
def efficientnetv2s_12channel(input_shape):
    input_tensor = tf.keras.layers.Input(shape=input_shape)

    channels = tf.split(input_tensor, num_or_size_splits=12, axis=-1)

    # Load EfficientNetV2S base model without top layers
    # weights=None means we train from scratch
    base_model = tf.keras.applications.EfficientNetV2S(include_top=False, input_tensor=input_tensor, weights=None)

    # Add custom top layers
    x = base_model.output
    x = tf.keras.layers.Conv2D(32, 3, padding='same')(x)
    x = tf.keras.layers.BatchNormalization()(x)
    x = tf.keras.layers.ReLU()(x)
    x = tf.keras.layers.GlobalAveragePooling2D()(x)
    x = tf.keras.layers.Dense(64*64, activation='sigmoid')(x)
    x = tf.keras.layers.Reshape((64, 64, 1))(x)

    model = tf.keras.models.Model(inputs=input_tensor, outputs=x)
    return model

# Initialize evaluation metrics for model assessment
accuracy = tf.keras.metrics.Accuracy()
precision = tf.keras.metrics.Precision()
auc = tf.keras.metrics.AUC()
recall = tf.keras.metrics.Recall()
binary_accuracy = tf.keras.metrics.BinaryAccuracy()
mae = tf.keras.metrics.MeanAbsoluteError()
auc_pr = tf.keras.metrics.AUC(curve='PR')
for i in range(k):
    print(f'Fold {i + 1}:')
    # Create history callback to track training metrics
    history_efficientnetv2s = tf.keras.callbacks.History()
    
    # Prepare training and validation data for current fold
    # Use 4 folds for training and 1 fold for validation
    not_i = [j for j in range(5) if j != i]
    validation_data = folds[i]
    training_data = folds[not_i[0]]
    training_data = training_data.concatenate(folds[not_i[1]])
    training_data = training_data.concatenate(folds[not_i[2]])
    training_data = training_data.concatenate(folds[not_i[3]])
    
    # Batch and prefetch data for better performance
    training_data = training_data.batch(100)
    training_data = training_data.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)
    validation_data = validation_data.batch(100)
    validation_data = validation_data.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)
    
    # Create and compile model
    model = efficientnetv2s_12channel((64,64,12))
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[accuracy, precision, auc, recall, binary_accuracy, mae])
    model.summary()
    model.fit(training_data, epochs=75, validation_data=validation_data, callbacks=[history_efficientnetv2s])
    
    # Evaluate model on validation data
    for batch in validation_data:
        inputs, targets = batch
        predictions = model.predict(inputs)
        targets = targets.numpy()
        predictions = predictions
        binary_accuracy.update_state(targets, predictions)
        auc.update_state(targets, predictions)
        auc_pr.update_state(targets, predictions)
        mae.update_state(targets, predictions)
        precision.update_state(targets, predictions)
        recall.update_state(targets, predictions)
    
    # Print evaluation metrics for current fold
    print("Binary Accuracy: ", binary_accuracy.result().numpy())
    print("AUC PR: ", auc_pr.result().numpy())
    print("AUC: ", auc.result().numpy())
    print("MAE: ", mae.result().numpy())
    print("Precision: ", precision.result().numpy())
    print("Recall: ", recall.result().numpy())
    
    # Plot training history for current fold
    plt.plot(history_efficientnetv2s.history['mean_absolute_error'])
    plt.plot(history_efficientnetv2s.history['val_mean_absolute_error'])
    plt.xlabel('Epochs')
    plt.ylabel('MAE Loss')
    plt.ylim(0,0.2)
    plt.legend(['Train', 'Test'], loc="upper right")
    plt.title("Epochs vs. MAE Loss: Efficient Net V2S")
    plt.show()
    plt.close()
    
    # Save training history for first fold
    if i == 0:
      with open('/content/drive/MyDrive/Data_History/history_efficientnetv2s.pickle', 'wb') as f:
        pickle.dump(history_efficientnetv2s.history, f)

"""## Create and implement RegNet"""



# Function to create RegNet model with 12 input channels
# RegNet is a design space for efficient CNNs
def regnet(input_shape):
    input_tensor = tf.keras.layers.Input(shape=input_shape)

    channels = tf.split(input_tensor, num_or_size_splits=12, axis=-1)

    base_model = tf.keras.applications.regnet.RegNetX080(include_top=False, input_tensor=input_tensor, weights=None)

    # Add custom top layers for fire prediction
    x = base_model.output
    x = tf.keras.layers.Conv2D(32, 3, padding='same')(x)
    x = tf.keras.layers.BatchNormalization()(x)
    x = tf.keras.layers.ReLU()(x)
    x = tf.keras.layers.GlobalAveragePooling2D()(x)
    x = tf.keras.layers.Dense(64*64, activation='sigmoid')(x)
    x = tf.keras.layers.Reshape((64, 64, 1))(x)

    model = tf.keras.models.Model(inputs=input_tensor, outputs=x)
    return model

# Initialize evaluation metrics
accuracy = tf.keras.metrics.Accuracy()
precision = tf.keras.metrics.Precision()
auc = tf.keras.metrics.AUC()
recall = tf.keras.metrics.Recall()
binary_accuracy = tf.keras.metrics.BinaryAccuracy()
mae = tf.keras.metrics.MeanAbsoluteError()
auc_pr = tf.keras.metrics.AUC(curve='PR')

# Train and evaluate model using k-fold cross validation
for i in range(k):
    print(f'Fold {i + 1}:')
    history_regnet = tf.keras.callbacks.History()
    not_i = [j for j in range(5) if j != i]
    validation_data = folds[i]
    training_data = folds[not_i[0]]
    training_data = training_data.concatenate(folds[not_i[1]])
    training_data = training_data.concatenate(folds[not_i[2]])
    training_data = training_data.concatenate(folds[not_i[3]])
    
    # Optimize data pipeline
    training_data = training_data.batch(100)
    training_data = training_data.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)
    validation_data = validation_data.batch(100)
    validation_data = validation_data.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)
    
    # Create and compile model
    model = regnet((64,64,12))
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[accuracy, precision, auc, recall, binary_accuracy, mae])
    model.summary()
    model.fit(training_data, epochs=75, validation_data=validation_data, callbacks=[history_regnet])
    
    # Evaluate model
    for batch in validation_data:
        inputs, targets = batch
        predictions = model.predict(inputs)
        targets = targets.numpy()
        predictions = predictions
        binary_accuracy.update_state(targets, predictions)
        auc.update_state(targets, predictions)
        auc_pr.update_state(targets, predictions)
        mae.update_state(targets, predictions)
        precision.update_state(targets, predictions)
        recall.update_state(targets, predictions)
    
    # Print metrics
    print("Binary Accuracy: ", binary_accuracy.result().numpy())
    print("AUC PR: ", auc_pr.result().numpy())
    print("AUC: ", auc.result().numpy())
    print("MAE: ", mae.result().numpy())
    print("Precision: ", precision.result().numpy())
    print("Recall: ", recall.result().numpy())
    
    # Plot training history
    plt.plot(history_regnet.history['mean_absolute_error'])
    plt.plot(history_regnet.history['val_mean_absolute_error'])
    plt.xlabel('Epochs')
    plt.ylabel('MAE Loss')
    plt.ylim(0,0.2)
    plt.legend(['Train', 'Test'], loc="upper right")
    plt.title("Epochs vs. MAE Loss: Regnet")
    plt.show()
    plt.close()
    
    # Save history for first fold
    if i == 0:
      with open('/content/drive/MyDrive/Data_History/history_regnet.pickle', 'wb') as f:
        pickle.dump(history_regnet.history, f)

"""## Create and implement VGG19"""

# Import required libraries
import pickle
import tensorflow as tf

# Function to create VGG19 model with 12 input channels
# VGG19 is a deep CNN architecture with 19 layers
def vgg19(input_shape):
    input_tensor = tf.keras.layers.Input(shape=input_shape)

    channels = tf.split(input_tensor, num_or_size_splits=input_shape[-1], axis=-1)

    # Load VGG19 base model without top layers
    base_model = tf.keras.applications.vgg19.VGG19(include_top=False, input_tensor=input_tensor, weights=None)

    # Add custom top layers for fire prediction
    x = base_model.output
    x = tf.keras.layers.Conv2D(32, 3, padding='same')(x)
    x = tf.keras.layers.BatchNormalization()(x)
    x = tf.keras.layers.ReLU()(x)
    x = tf.keras.layers.GlobalAveragePooling2D()(x)
    x = tf.keras.layers.Dense(input_shape[0]*input_shape[1], activation='sigmoid')(x)
    x = tf.keras.layers.Reshape((input_shape[0], input_shape[1], 1))(x)

    model = tf.keras.models.Model(inputs=input_tensor, outputs=x)
    return model

# Initialize evaluation metrics
accuracy = tf.keras.metrics.Accuracy()
precision = tf.keras.metrics.Precision()
auc = tf.keras.metrics.AUC()
recall = tf.keras.metrics.Recall()
binary_accuracy = tf.keras.metrics.BinaryAccuracy()
mae = tf.keras.metrics.MeanAbsoluteError()
auc_pr = tf.keras.metrics.AUC(curve='PR')

# Train and evaluate model using k-fold cross validation
for i in range(k):
    print(f'Fold {i + 1}:')
    history_vgg19 = tf.keras.callbacks.History()
    
    # Prepare training and validation data
    not_i = [j for j in range(5) if j != i]
    validation_data = folds[i]
    training_data = folds[not_i[0]]
    training_data = training_data.concatenate(folds[not_i[1]])
    training_data = training_data.concatenate(folds[not_i[2]])
    training_data = training_data.concatenate(folds[not_i[3]])
    
    # Optimize data pipeline
    training_data = training_data.batch(100)
    training_data = training_data.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)
    validation_data = validation_data.batch(100)
    validation_data = validation_data.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)
    
    # Create and compile model
    model = vgg19((64,64,12))
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[accuracy, precision, auc, recall, binary_accuracy, mae])
    model.fit(training_data, epochs=75, validation_data=validation_data, callbacks=[history_vgg19])
    
    # Evaluate model
    for batch in validation_data:
        inputs, targets = batch
        predictions = model.predict(inputs)
        targets = targets.numpy()
        predictions = predictions
        # Update metrics
        binary_accuracy.update_state(targets, predictions)
        auc.update_state(targets, predictions)
        auc_pr.update_state(targets, predictions)
        mae.update_state(targets, predictions)
        precision.update_state(targets, predictions)
        recall.update_state(targets, predictions)
    
    # Print metrics
    print("Binary Accuracy: ", binary_accuracy.result().numpy())
    print("AUC PR: ", auc_pr.result().numpy())
    print("AUC: ", auc.result().numpy())
    print("MAE: ", mae.result().numpy())
    print("Precision: ", precision.result().numpy())
    print("Recall: ", recall.result().numpy())
    
    # Plot training history
    plt.plot(history_vgg19.history['mean_absolute_error'])
    plt.plot(history_vgg19.history['val_mean_absolute_error'])
    plt.xlabel('Epochs')
    plt.ylabel('MAE Loss')
    plt.ylim(0,0.2)
    plt.legend(['Train', 'Test'], loc="upper right")
    plt.title("Epochs vs. MAE Loss: Inception")
    plt.show()
    plt.close()
    
    # Save history for first fold
    if i == 0:
      with open('/content/drive/MyDrive/Data_History/history_vgg19.pickle', 'wb') as f:
        pickle.dump(history_vgg19.history, f)

"""## Create and Implement Logistic Regression"""

import tensorflow as tf

# Define the input and output shapes
input_shape = (64, 64, 12)
output_shape = (64, 64, 1)

# Define the model architecture
model = tf.keras.Sequential([
    tf.keras.layers.InputLayer(input_shape=input_shape),
    tf.keras.layers.Flatten(),  # Flatten input to 1D
    tf.keras.layers.Dense(output_shape[0] * output_shape[1], activation='sigmoid'),  # Single dense layer
    tf.keras.layers.Reshape(output_shape)  # Reshape to match output dimensions
])

# Define loss function and optimizer
# Binary crossentropy is standard for binary classification
loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=False)
optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)

# Initialize metrics for evaluation
accuracy = tf.keras.metrics.Accuracy()
precision = tf.keras.metrics.Precision()
auc = tf.keras.metrics.AUC()
recall = tf.keras.metrics.Recall()
binary_accuracy = tf.keras.metrics.BinaryAccuracy()
mae = tf.keras.metrics.MeanAbsoluteError()
auc_pr = tf.keras.metrics.AUC(curve='PR')

# Train and evaluate logistic regression model
for i in range(k):
    print(f'Fold {i + 1}:')
    history_lgregression = tf.keras.callbacks.History()
    
    # Prepare data for current fold
    not_i = [j for j in range(5) if j != i]
    validation_data = folds[i]
    training_data = folds[not_i[0]]
    training_data = training_data.concatenate(folds[not_i[1]])
    training_data = training_data.concatenate(folds[not_i[2]])
    training_data = training_data.concatenate(folds[not_i[3]])
    
    # Optimize data pipeline
    training_data = training_data.batch(100)
    training_data = training_data.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)
    validation_data = validation_data.batch(100)
    validation_data = validation_data.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)
    
    # Compile and train model
    model.compile(optimizer=optimizer, loss=loss_fn, 
                 metrics=[accuracy, precision, auc, recall, binary_accuracy, mae])
    model.fit(training_data, epochs=75, validation_data=validation_data, 
             callbacks=[history_lgregression])
    
    # Evaluate model
    for batch in validation_data:
        inputs, targets = batch
        predictions = model.predict(inputs)
        targets = targets.numpy()
        predictions = predictions
        
        # Update metrics
        binary_accuracy.update_state(targets, predictions)
        auc.update_state(targets, predictions)
        auc_pr.update_state(targets, predictions)
        mae.update_state(targets, predictions)
        precision.update_state(targets, predictions)
        recall.update_state(targets, predictions)
    
    # Print metrics
    print("Binary Accuracy: ", binary_accuracy.result().numpy())
    print("AUC PR: ", auc_pr.result().numpy())
    print("AUC: ", auc.result().numpy())
    print("MAE: ", mae.result().numpy())
    print("Precision: ", precision.result().numpy())
    print("Recall: ", recall.result().numpy())
    
    # Plot training history
    plt.plot(history_lgregression.history['mean_absolute_error'])
    plt.plot(history_lgregression.history['val_mean_absolute_error'])
    plt.xlabel('Epochs')
    plt.ylabel('MAE Loss')
    plt.ylim(0,0.2)
    plt.legend(['Train', 'Test'], loc="upper right")
    plt.title("Epochs vs. MAE Loss: Logistic Regression")
    plt.show()
    plt.close()
    
    # Save history for first fold
    if i == 0:
      with open('/content/drive/MyDrive/Data_History/history_lgregression.pickle', 'wb') as f:
        pickle.dump(history_lgregression.history, f)

"""## Create and Implement Random Forest"""

# Import required libraries for Random Forest
import tensorflow as tf
from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier
from sklearn.metrics import mean_absolute_error, roc_auc_score, accuracy_score

# Define the Random Forest model
# n_estimators=100: number of trees in the forest
# max_depth=10: maximum depth of each tree
# random_state=42: for reproducibility
rf_model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)

# Function to train Random Forest model
# Handles data flattening and model training
def train_step(inputs, targets):
    # Flatten the inputs and targets for Random Forest
    # Reshape from (batch_size, 64, 64, channels) to (batch_size*64*64, channels)
    inputs_flat = tf.reshape(inputs, [-1, 64*64*12])
    targets_flat = tf.reshape(targets, [-1, 64*64])

    # Fit the Random Forest model
    rf_model.fit(inputs_flat, targets_flat)

    return rf_model

# Function to evaluate Random Forest model
# Calculates various evaluation metrics
def test_step(inputs, targets):
    # Flatten the inputs and targets
    inputs_flat = tf.reshape(inputs, [-1, 64*64*12])
    targets_flat = tf.reshape(targets, [-1, 64*64])

    # Make predictions
    output_flat = rf_model.predict(inputs_flat)

    # Calculate evaluation metrics:
    # - MAE: mean absolute error
    # - AUC: area under ROC curve
    # - Binary accuracy: accuracy for binary classification
    mae = mean_absolute_error(targets_flat, output_flat)
    auc = roc_auc_score(targets_flat, output_flat)
    binary_accuracy = accuracy_score(targets_flat > 0.5, output_flat > 0.5)

    return mae, auc, binary_accuracy

# Prepare data for Random Forest
# Combine folds for training and validation
validation_data = folds[0]
training_data = folds[1]
training_data = training_data.concatenate(folds[2])
training_data = training_data.concatenate(folds[3])
training_data = training_data.concatenate(folds[4])
training_data = training_data.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)
validation_data = validation_data.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)

# Train Random Forest model
# Run training for 10 epochs
for epoch in range(10):
    for inputs, targets in training_data:
        rf_model = train_step(inputs, targets)
    print("Epoch")

# Evaluate Random Forest model
# Collect metrics for each batch
mae_list = []
auc_list = []
binary_accuracy_list = []
for inputs, targets in validation_data:
    mae, auc, binary_accuracy = test_step(inputs, targets)
    mae_list.append(mae)
    auc_list.append(auc)
    binary_accuracy_list.append(binary_accuracy)

# Calculate average metrics across all batches
mae = tf.reduce_mean(tf.stack(mae_list))
auc = tf.reduce_mean(tf.stack(auc_list))
binary_accuracy = tf.reduce_mean(tf.stack(binary_accuracy_list))

"""## Combine Into One Graph"""

# Load training histories for visualization
# Load saved training histories for all models
import pickle
import matplotlib.pyplot as plt
with open('/content/drive/MyDrive/Data_History/history_vgg19.pickle', 'rb') as f:
    history_vgg19 = pickle.load(f)
with open('/content/drive/MyDrive/Data_History/history_resnet.pickle', 'rb') as f:
    history_resnet50 = pickle.load(f)
with open('/content/drive/MyDrive/Data_History/history_resnet101.pickle', 'rb') as f:
    history_resnet101 = pickle.load(f)
with open('/content/drive/MyDrive/Data_History/history_regnet.pickle', 'rb') as f:
    history_regnet = pickle.load(f)
with open('/content/drive/MyDrive/Data_History/history_efficientnet.pickle', 'rb') as f:
    history_efficientnet = pickle.load(f)
with open('/content/drive/MyDrive/Data_History/history_efficientnetv2s.pickle', 'rb') as f:
    history_efficientnetv2s = pickle.load(f)

# Create comparison plots for all models
# Create figure with two subplots: training and validation MAE
fig,axes = plt.subplots(nrows=1, ncols=2, figsize=(15,5))

# Plot training MAE for all models
axes[0].plot(history_resnet50['mean_absolute_error'])
axes[0].plot(history_resnet101['mean_absolute_error'])
axes[0].plot(history_efficientnet['mean_absolute_error'])
axes[0].plot(history_efficientnetv2s['mean_absolute_error'])
axes[0].plot(history_regnet['mean_absolute_error'])
axes[0].plot(history_vgg19['mean_absolute_error'])
axes[0].legend(['ResNet50','ResNet101', 'EfficientNet', 'EfficientNetV2S', 'RegNet', 'VGG19'], 
               loc="upper right")
axes[0].set_xlabel('Epochs')
axes[0].set_ylabel('MAE Loss')
axes[0].set_ylim(0, 0.3)
axes[0].set_title("Training")

# Plot validation MAE for all models
axes[1].plot(history_resnet50['val_mean_absolute_error'])
axes[1].plot(history_resnet101['val_mean_absolute_error'])
axes[1].plot(history_efficientnet['val_mean_absolute_error'])
axes[1].plot(history_efficientnetv2s['val_mean_absolute_error'])
axes[1].plot(history_regnet['val_mean_absolute_error'])
axes[1].plot(history_vgg19['val_mean_absolute_error'])
axes[1].legend(['ResNet50','ResNet101', 'EfficientNet', 'EfficientNetV2S', 'RegNet', 'VGG19'], 
               loc="upper right")
axes[1].set_xlabel('Epochs')
axes[1].set_ylabel('MAE Loss')
axes[1].set_ylim(0, 0.3)
axes[1].set_title("Testing")

# Adjust layout and save figure
fig.subplots_adjust(wspace=0.2, hspace=0.5)
fig.suptitle("Epochs vs. MAE Loss")
plt.show()
plt.close()
fig.savefig("/content/drive/MyDrive/table6.jpg")
